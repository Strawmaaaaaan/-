# 学习笔记

供自用

## 目录

## 机器学习基础

### 损失

#### 1. softmax Loss
$$ $$
  

#### 2. Cross-Entropy Loss
交叉熵损失用来描述两个概率分布之间的距离，熵越大说明预测结果越混乱具有不确定性，熵越小则说明结果越准确。计算公式为：
$$L = -\frac{1}{N}\sum_{i}\sum_{c=1}^{M}y_{ic}log(p_{ic})$$
其中：
- $M$代表类别数
- $y_{ic}$代表符号函数，如果样本$i$的真实类别等于$c$则取1，否则取0
- $p_{ic}$代表样本$i$属于类别$c$的预测概率

