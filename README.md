# 学习笔记

供自用

## 目录

## 机器学习基础

### 损失

#### 1. softmax Loss
$$ $$
  

#### 2. Cross-Entropy Loss
交叉熵损失用来描述两个概率分布之间的距离，熵越大说明预测结果越混乱具有不确定性，熵越小则说明结果越准确。
计算公式为：
$$L = -/frac{1}{N}\sum_{i}\sum_{c=1}^{M}y_{ic}log(p_{ic})$$

